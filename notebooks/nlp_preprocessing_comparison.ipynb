{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Preprocessing Comparison Suite\n",
        "\n",
        "SP26‑CSC115‑N850: Machine Learning I  \n",
        "Instructor: Professor Ann Aksut  \n",
        "Student: Ericka‑Lee DeMaria  \n",
        "Date: <Insert Date>  \n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook demonstrates a complete natural language processing (NLP) preprocessing workflow in Python.  \n",
        "It compares tokenization, stop‑word removal, stemming, lemmatization, part‑of‑speech (POS) tagging, and spaCy‑based linguistic analysis.  \n",
        "The goal is to understand how different preprocessing techniques transform text and how these choices affect downstream machine learning tasks.\n",
        "\n",
        "This notebook follows the CSC‑115 rubric and incorporates prior feedback on clarity, structure, documentation, and interpretability.\n"
      ],
      "metadata": {
        "id": "8vx4iVmuFMmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Preprocessing Comparison Suite\n",
        "\n",
        "This notebook demonstrates how different NLP preprocessing techniques transform text.  \n",
        "It compares:\n",
        "\n",
        "- Tokenization  \n",
        "- Stop‑word removal  \n",
        "- Stemming (Porter, Lancaster, Snowball)  \n",
        "- Lemmatization (noun vs verb)  \n",
        "- POS tagging  \n",
        "- spaCy tokenization, lemmatization, and POS tagging  \n",
        "- A summary comparison table  \n",
        "- A linguistic difficulty score  \n",
        "\n",
        "This notebook is part of the *NLP Preprocessing Comparison Project*.\n"
      ],
      "metadata": {
        "id": "ms_7pEu-DjMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Pipeline Overview\n",
        "\n",
        "The preprocessing pipeline used in this notebook is:\n",
        "\n",
        "Raw Text  \n",
        "→ Tokenization  \n",
        "→ Stop‑Word Removal  \n",
        "→ Stemming (Porter, Lancaster, Snowball)  \n",
        "→ Lemmatization (noun vs verb)  \n",
        "→ POS Tagging  \n",
        "→ spaCy Pipeline (tokens, lemmas, POS)  \n",
        "→ Summary Comparison Table  \n",
        "→ Difficulty Score\n"
      ],
      "metadata": {
        "id": "pATvQV3sFVXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and environment setup\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download required NLTK resources.\n",
        "# quiet=True suppresses verbose download messages for a cleaner notebook.\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
        "\n",
        "# Load the small English spaCy model.\n",
        "# This model provides tokenization, POS tagging, and lemmatization.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "_U1a0-MsIjp6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSC‑115 States\n",
        "\n",
        "### Problem State\n",
        "The problem is to preprocess natural language text using multiple NLP techniques and compare their outputs.  \n",
        "This supports understanding of how preprocessing affects model performance, interpretability, and robustness.\n",
        "\n",
        "### System State\n",
        "The system consists of:\n",
        "- Python 3.x\n",
        "- nltk for classical NLP tools\n",
        "- spaCy for modern NLP pipelines\n",
        "- pandas for tabular comparison\n",
        "\n",
        "All required corpora and models are downloaded at runtime.\n",
        "\n",
        "### Data State\n",
        "The data is a single input sentence provided as a string.  \n",
        "The focus is on transformation and analysis rather than training a predictive model.\n",
        "\n",
        "### Model State\n",
        "No predictive model is trained.  \n",
        "Instead, the notebook applies:\n",
        "- NLTK stemmers (Porter, Lancaster, Snowball)\n",
        "- NLTK WordNet lemmatizer\n",
        "- NLTK POS tagger\n",
        "- spaCy linguistic pipeline\n",
        "\n",
        "### Evaluation State\n",
        "Evaluation is qualitative and comparative.  \n",
        "A summary table and a custom difficulty score are used to analyze differences across preprocessing methods.\n"
      ],
      "metadata": {
        "id": "QTF62iwBL0TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input sentence for analysis.\n",
        "# This sentence contains verbs, adjectives, and a noun, which makes it useful\n",
        "# for demonstrating stemming, lemmatization, and POS tagging.\n",
        "\n",
        "sentence = \"We went again and had an even better experience!\"\n",
        "sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CG8SSpgML8vf",
        "outputId": "6bad0183-7a7b-4ba1-e404-089de61e3701"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'We went again and had an even better experience!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenization using NLTK's word_tokenize.\n",
        "# This splits the raw sentence into individual tokens (words and punctuation).\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Display the list of tokens.\n",
        "tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "MGAnhTByL_q0",
        "outputId": "b203807e-ef72-4f00-d489-dcd3201a7f76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-973865628.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This splits the raw sentence into individual tokens (words and punctuation).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the list of tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Tokenization Interpretation</summary>\n",
        "\n",
        "Tokenization splits the sentence into discrete units that can be processed by algorithms.  \n",
        "Punctuation is preserved as separate tokens. This step is foundational for all subsequent NLP operations.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "ayqBRxGiMFnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Stop‑word removal.\n",
        "# Stop words are high‑frequency words (e.g., \"and\", \"the\") that often do not add\n",
        "# significant semantic meaning for many NLP tasks.\n",
        "\n",
        "# Load the English stop‑word list from NLTK.\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Filter out tokens that are stop words.\n",
        "# The .lower() call ensures case‑insensitive comparison.\n",
        "filtered_tokens = [t for t in tokens if t.lower() not in stop_words]\n",
        "\n",
        "# Display the filtered tokens.\n",
        "filtered_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oyxMJ0CNMKGi",
        "outputId": "f3b61269-1b21-4d0e-ed62-73c9fcf6e9b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3156585974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Filter out tokens that are stop words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The .lower() call ensures case‑insensitive comparison.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Display the filtered tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Stop‑Word Removal Interpretation</summary>\n",
        "\n",
        "Stop‑word removal reduces noise and dimensionality by removing common words that typically do not contribute to the core meaning.  \n",
        "This can improve efficiency and sometimes model performance, especially in bag‑of‑words or frequency‑based representations.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "_T4KdGqsMOvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Stemming comparison.\n",
        "# Stemming reduces words to crude base forms by stripping prefixes and suffixes.\n",
        "# Different stemmers use different rules and levels of aggressiveness.\n",
        "\n",
        "# Initialize three common NLTK stemmers.\n",
        "stemmers = {\n",
        "    \"Porter\": PorterStemmer(),\n",
        "    \"Lancaster\": LancasterStemmer(),\n",
        "    \"Snowball\": SnowballStemmer(\"english\")\n",
        "}\n",
        "\n",
        "# For each stemmer, compute the stemmed version of each filtered token.\n",
        "stem_results = {\n",
        "    name: [stemmer.stem(t) for t in filtered_tokens]\n",
        "    for name, stemmer in stemmers.items()\n",
        "}\n",
        "\n",
        "# Display the dictionary of stemmed outputs.\n",
        "stem_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "nmOrFy91MSkN",
        "outputId": "5b6a3811-f947-41c6-abc1-fb1bee35585c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1995965373.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# For each stemmer, compute the stemmed version of each filtered token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m stem_results = {\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstemmers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Stemming Interpretation</summary>\n",
        "\n",
        "Stemming reduces words to base forms using heuristic rules.  \n",
        "- Porter is relatively conservative and often produces stems close to real words.  \n",
        "- Lancaster is more aggressive and may produce shorter, less interpretable stems.  \n",
        "- Snowball is a balanced, multilingual stemmer.\n",
        "\n",
        "Differences between stemmers illustrate how algorithm choice can affect feature representations in NLP models.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "fCd4sXR4MWDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Lemmatization comparison.\n",
        "# Lemmatization reduces words to their dictionary base forms (lemmas),\n",
        "# using vocabulary and morphological analysis. It is sensitive to part‑of‑speech.\n",
        "\n",
        "# Initialize the WordNet lemmatizer.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize tokens assuming they are nouns.\n",
        "lemmas_n = [lemmatizer.lemmatize(t, pos=\"n\") for t in filtered_tokens]\n",
        "\n",
        "# Lemmatize tokens assuming they are verbs.\n",
        "lemmas_v = [lemmatizer.lemmatize(t, pos=\"v\") for t in filtered_tokens]\n",
        "\n",
        "# Store results in a dictionary for clarity.\n",
        "lemmatization_results = {\n",
        "    \"Lemma (n)\": lemmas_n,\n",
        "    \"Lemma (v)\": lemmas_v\n",
        "}\n",
        "\n",
        "# Display the lemmatization results.\n",
        "lemmatization_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bRSaE-jsMZnd",
        "outputId": "cfcdd116-332d-4390-862f-282ec2c85e60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1663971891.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Lemmatize tokens assuming they are nouns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlemmas_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Lemmatize tokens assuming they are verbs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Lemmatization Interpretation</summary>\n",
        "\n",
        "Lemmatization uses dictionary knowledge and part‑of‑speech information to produce valid base forms.  \n",
        "When treated as verbs, irregular forms such as \"went\" and \"had\" are correctly mapped to \"go\" and \"have\".  \n",
        "When treated as nouns, many words remain unchanged because they are already in their base noun form.\n",
        "\n",
        "This demonstrates that lemmatization quality depends heavily on accurate POS information.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "dfVfxuI1Mlhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: POS tagging using NLTK.\n",
        "# POS tagging assigns a part‑of‑speech label (e.g., noun, verb, adjective) to each token.\n",
        "\n",
        "pos_tags = pos_tag(filtered_tokens)\n",
        "\n",
        "# Display the list of (token, POS tag) pairs.\n",
        "pos_tags\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "A6KOFNbHMiKI",
        "outputId": "3ade3f0f-9bcd-4d6c-b95e-7e838232558c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3466740398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# POS tagging assigns a part‑of‑speech label (e.g., noun, verb, adjective) to each token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the list of (token, POS tag) pairs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>POS Tagging Interpretation</summary>\n",
        "\n",
        "POS tags provide grammatical structure to the sentence.  \n",
        "These tags can be used to improve lemmatization, filter specific word types, or engineer features for downstream models.  \n",
        "For example, distinguishing verbs from nouns helps ensure that lemmatization uses the correct base form.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "dA888asTMoYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: spaCy pipeline.\n",
        "# spaCy provides an integrated pipeline for tokenization, POS tagging, and lemmatization.\n",
        "\n",
        "# Process the original sentence with spaCy.\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract spaCy tokens, lemmas, and POS tags.\n",
        "spacy_tokens = [t.text for t in doc]\n",
        "spacy_lemmas = [t.lemma_ for t in doc]\n",
        "spacy_pos = [t.pos_ for t in doc]\n",
        "\n",
        "# Display spaCy results in a dictionary for readability.\n",
        "{\n",
        "    \"spaCy Tokens\": spacy_tokens,\n",
        "    \"spaCy Lemmas\": spacy_lemmas,\n",
        "    \"spaCy POS\": spacy_pos\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af5tmfC-Mr3P",
        "outputId": "279b0216-968a-41e5-d707-56055ea0359c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spaCy Tokens': ['We',\n",
              "  'went',\n",
              "  'again',\n",
              "  'and',\n",
              "  'had',\n",
              "  'an',\n",
              "  'even',\n",
              "  'better',\n",
              "  'experience',\n",
              "  '!'],\n",
              " 'spaCy Lemmas': ['we',\n",
              "  'go',\n",
              "  'again',\n",
              "  'and',\n",
              "  'have',\n",
              "  'an',\n",
              "  'even',\n",
              "  'well',\n",
              "  'experience',\n",
              "  '!'],\n",
              " 'spaCy POS': ['PRON',\n",
              "  'VERB',\n",
              "  'ADV',\n",
              "  'CCONJ',\n",
              "  'VERB',\n",
              "  'DET',\n",
              "  'ADV',\n",
              "  'ADJ',\n",
              "  'NOUN',\n",
              "  'PUNCT']}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>spaCy Interpretation</summary>\n",
        "\n",
        "spaCy provides a modern, production‑grade NLP pipeline.  \n",
        "Its tokenization, POS tagging, and lemmatization are trained on large corpora and often outperform rule‑based approaches.  \n",
        "Comparing spaCy outputs with NLTK results highlights differences between classical and modern NLP tools.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "SM8wwTsuMsnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Build a summary comparison table using pandas.\n",
        "# This table aligns each filtered token with its stemmed and lemmatized forms,\n",
        "# as well as POS tags from NLTK and spaCy.\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Word\": filtered_tokens,\n",
        "    \"Porter\": stem_results[\"Porter\"],\n",
        "    \"Lancaster\": stem_results[\"Lancaster\"],\n",
        "    \"Snowball\": stem_results[\"Snowball\"],\n",
        "    \"Lemma (n)\": lemmas_n,\n",
        "    \"Lemma (v)\": lemmas_v,\n",
        "    \"POS (NLTK)\": [p[1] for p in pos_tags],\n",
        "    \"spaCy Lemma\": spacy_lemmas[:len(filtered_tokens)],\n",
        "    \"spaCy POS\": spacy_pos[:len(filtered_tokens)]\n",
        "})\n",
        "\n",
        "# Display the comparison table.\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "x_HVi_I5M1Ln",
        "outputId": "18bac10c-daa6-4937-b5f5-7ca6fe3d4e8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-783944102.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m df = pd.DataFrame({\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"Porter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstem_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Porter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"Lancaster\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstem_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Lancaster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Summary Table Interpretation</summary>\n",
        "\n",
        "The summary table consolidates all preprocessing outputs for direct comparison.  \n",
        "It shows how each algorithm transforms the same token, making it easier to reason about:\n",
        "\n",
        "- Which stemmer is most aggressive  \n",
        "- How lemmatization differs by POS assumption  \n",
        "- How NLTK and spaCy POS tags align or differ  \n",
        "\n",
        "This table supports both qualitative analysis and potential feature engineering decisions.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "PHWpyUu0M7xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Compute a custom linguistic difficulty score.\n",
        "# This is a heuristic metric that combines:\n",
        "# - Number of filtered tokens\n",
        "# - Number of unique POS tags (NLTK)\n",
        "# - Number of unique values across all transformation columns\n",
        "\n",
        "num_tokens = len(filtered_tokens)\n",
        "num_unique_pos = len(set([p[1] for p in pos_tags]))\n",
        "num_unique_values = df.nunique().sum()\n",
        "\n",
        "difficulty_score = num_tokens + num_unique_pos + num_unique_values\n",
        "\n",
        "difficulty_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "WieS0x-YM9Mm",
        "outputId": "0b51de1d-a69d-4187-e2ac-7f44f89f7f3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2810551777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# - Number of unique values across all transformation columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnum_unique_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_unique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Difficulty Score Interpretation</summary>\n",
        "\n",
        "The difficulty score is a composite metric designed for exploratory analysis.  \n",
        "It increases with:\n",
        "- More tokens (longer text)\n",
        "- Greater POS diversity (more complex grammar)\n",
        "- More unique transformed values (greater variation across preprocessing methods)\n",
        "\n",
        "While not a formal linguistic metric, it provides a simple way to compare relative complexity across sentences.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "9JVXF9tPNBNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection\n",
        "\n",
        "This notebook implemented a complete NLP preprocessing workflow and compared multiple approaches using NLTK and spaCy.  \n",
        "I observed that different stemmers produce noticeably different outputs, with Lancaster being the most aggressive.  \n",
        "Lemmatization quality depended strongly on the assumed part‑of‑speech, reinforcing the importance of accurate POS tagging.  \n",
        "The spaCy pipeline provided consistent and modern linguistic analysis, which often aligned with but sometimes differed from NLTK outputs.\n",
        "\n",
        "The summary comparison table made it easier to interpret how each technique transformed the text.  \n",
        "The custom difficulty score added a quantitative perspective, even though it is heuristic.  \n",
        "\n",
        "This work aligns with previous feedback emphasizing:\n",
        "- Clear structure and sectioning\n",
        "- Inline explanations and interpretation\n",
        "- Reproducible, well‑documented code\n",
        "- Explicit connection between methods and their impact\n",
        "\n",
        "Future extensions could include:\n",
        "- Named entity recognition (NER)\n",
        "- Dependency parsing\n",
        "- Applying this preprocessing pipeline to a larger corpus and training a downstream classifier.\n"
      ],
      "metadata": {
        "id": "8RJzKjPONHxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- SP26‑CSC115‑N850: Machine Learning I, zyBooks, Chapter 2.4 (Natural Language Processing)  \n",
        "- NLTK Documentation: https://www.nltk.org/  \n",
        "- spaCy Documentation: https://spacy.io/  \n",
        "- WordNet Lexical Database: https://wordnet.princeton.edu/  \n"
      ],
      "metadata": {
        "id": "oeNcpLcuNI-C"
      }
    }
  ]
}